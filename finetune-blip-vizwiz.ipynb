{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21d6486-1922-4c31-99e0-eacd4b93b52f",
   "metadata": {},
   "source": [
    "# CS 5330: Final Project\n",
    "### VQA finetuning of BLIP using Vizwiz dataset\n",
    "- *Model link*: https://huggingface.co/Salesforce/blip-vqa-base\n",
    "- *Dataset link*: https://www.kaggle.com/datasets/lhanhsin/vizwiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e496ac0-3e0a-41c2-ba52-498649abf6ac",
   "metadata": {},
   "source": [
    "Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc97a68-b08b-470f-8789-9b1454061da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b7f7b-1bbd-4245-9754-bac2bd79a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"lhanhsin/vizwiz\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f92a07-8fe5-4803-8e34-0d11ce61fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30829724-84d9-45af-9658-b86c3c591bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 50631  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94006e3a-a778-4c5b-a3a3-1763c92d88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = '/home/kumar.riti/.cache/kagglehub/datasets/lhanhsin/vizwiz/versions/2/'\n",
    "train_annotations_path = path + \"Annotations/Annotations/train.json\" \n",
    "val_annotations_path = path + \"Annotations/Annotations/val.json\"  \n",
    "image_dir = \"train/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e3a3c-0f11-4032-8933-dadaedb3006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_annotations_path, 'r') as f:\n",
    "    train = json.load(f)\n",
    "train_df = pd.DataFrame(train)\n",
    "\n",
    "with open(val_annotations_path, 'r') as f:\n",
    "    val = json.load(f)\n",
    "\n",
    "val_df = pd.DataFrame(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1bb5b3-46a0-455c-aa17-7a85b2e534ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_confident_answer(answers):\n",
    "    if isinstance(answers, list) and len(answers) > 0:\n",
    "        return max(answers, key=lambda x: x['answer_confidence'])['answer']\n",
    "    return \"\"\n",
    "\n",
    "train_df['most_confident_answer'] = train_df['answers'].apply(get_most_confident_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d701265-e926-4a99-ae85-53a4800c8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['most_confident_answer'] = val_df['answers'].apply(get_most_confident_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abac29b-7df9-42dd-ae11-0da44a02fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "trian_df = train_df[train_df['answerable'] == 1]\t\n",
    "val_df = val_df[val_df['answerable'] == 1]\n",
    "train_df = train_df[:]\n",
    "val_df = val_df[:]\n",
    "\n",
    "val_df = val_df[['image', 'question', 'most_confident_answer']]\n",
    "train_df = train_df[['image', 'question', 'most_confident_answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b9b47-b19f-49f2-bc71-5f1406fff1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752101b4-bdfc-4a59-a425-ce2cb2c90ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"VQA dataset using a pandas DataFrame.\"\"\"\n",
    "\n",
    "    def __init__(self, df, processor, prefix, image_size=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing 'question', 'answer', and 'image' columns.\n",
    "            processor: Hugging Face processor for tokenization and feature extraction.\n",
    "            prefix: for path\n",
    "            image_size: for resizing\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.prefix = prefix\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),  \n",
    "            transforms.ToTensor(), \n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.df.iloc[idx]\n",
    "        question = row['question']\n",
    "        answer = row['most_confident_answer']\n",
    "        image_path = self.prefix + row['image']\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        \n",
    "\n",
    "        # Process the image and text\n",
    "        encoding = self.processor(image, question, padding=\"max_length\",max_length=8, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Process the answer\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            answer, max_length=8, pad_to_max_length=True, return_tensors='pt'\n",
    "        )\n",
    "        encoding[\"labels\"] = labels\n",
    "\n",
    "        # Remove batch dimension\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "\n",
    "        return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4441b2-bc32-4a94-b835-6b27c135e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = VQADataset(df=train_df,\n",
    "                          processor=processor, prefix=f'{path}/train/train/')\n",
    "valid_dataset = VQADataset(df=val_df,\n",
    "                          processor=processor, prefix=f'{path}/val/val/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2732c60-4123-4a83-b619-d48489b1d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n",
    "\n",
    "num_epochs = 1\n",
    "info = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx, batch in zip(tqdm(range(len(train_dataloader)), desc='Training batch: ...'), train_dataloader):\n",
    "        input_ids = batch.pop('input_ids').to(device)\n",
    "        pixel_values = batch.pop('pixel_values').to(device)\n",
    "        attention_masked = batch.pop('attention_mask').to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        attention_mask=attention_masked,\n",
    "                        labels=labels)\n",
    "            \n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for idx, batch in zip(tqdm(range(len(valid_dataloader)), desc='Validating batch: ...'), valid_dataloader):\n",
    "        input_ids = batch.pop('input_ids').to(device)\n",
    "        pixel_values = batch.pop('pixel_values').to(device)\n",
    "        attention_masked = batch.pop('attention_mask').to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        attention_mask=attention_masked,\n",
    "                        labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "    info.append((epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    print(\"Epoch: {} - Training loss: {} - Eval Loss: {} - LR: {}\".format(epoch+1, epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    scheduler.step()\n",
    "    model.save_pretrained(\"Model/blip-finetuned-vizwiz\", from_pt=True) \n",
    "    processor.save_pretrained(\"Model/blip-finetuned-vizwiz\", from_pt=True) \n",
    "    print(\"Saved model to Model/blip-finetuned-vizwiz\")\n",
    "print(\"The finetuning process is completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fc397-fe91-4229-815f-2bc8ec95f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=8, shuffle=False, pin_memory=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n",
    "\n",
    "num_epochs = 1\n",
    "info = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx, batch in zip(tqdm(range(len(train_dataloader)), desc='Training batch: ...'), train_dataloader):\n",
    "        input_ids = batch.pop('input_ids').to(device)\n",
    "        pixel_values = batch.pop('pixel_values').to(device)\n",
    "        attention_masked = batch.pop('attention_mask').to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        attention_mask=attention_masked,\n",
    "                        labels=labels)\n",
    "            \n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for idx, batch in zip(tqdm(range(len(valid_dataloader)), desc='Validating batch: ...'), valid_dataloader):\n",
    "        input_ids = batch.pop('input_ids').to(device)\n",
    "        pixel_values = batch.pop('pixel_values').to(device)\n",
    "        attention_masked = batch.pop('attention_mask').to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        attention_mask=attention_masked,\n",
    "                        labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "    info.append((epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    print(\"Epoch: {} - Training loss: {} - Eval Loss: {} - LR: {}\".format(epoch+1, epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    scheduler.step()\n",
    "    \n",
    "    model.save_pretrained(\"Model/blip-saved-model\", from_pt=True) \n",
    "    processor.save_pretrained(\"Model/blip-saved-model\", from_pt=True) \n",
    "    print(\"Saved model to Model/blip-saved-model\")\n",
    "        \n",
    "    \n",
    "pickle.dump(info, open(\"tracking_information.pkl\", \"wb\"))\n",
    "print(\"The finetuning process is completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac58197-0484-4d49-ae2c-5ed8a9ae4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def visualize_vqa_result(processor, model, dataset, index):\n",
    "    \"\"\"\n",
    "    Visualizes the VQA results by displaying the image, question, actual answer, and predicted answer.\n",
    "\n",
    "    Args:\n",
    "        processor: BLIP processor for encoding images and text.\n",
    "        model: Fine-tuned BLIP model.\n",
    "        dataset: Dataset containing \"image\", \"question\", and \"answer\".\n",
    "        index: Index of the sample to visualize.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    image_path = path + 'val/val/' + dataset[\"image\"][index]\n",
    "    question = dataset[\"question\"][index]\n",
    "    actual_answer = dataset[\"most_confident_answer\"][index]\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Test Image\")\n",
    "    plt.show()\n",
    "\n",
    "    inputs = processor(images=image, text=question, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs)\n",
    "    predicted_answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Actual Answer: {actual_answer}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a711b7-8524-44e5-9154-f11e98e84312",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_vqa_result(processor, model, val_df, index=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55956337-8dc0-4b04-b3be-374fbd4d31e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
